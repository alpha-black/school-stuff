%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%% thesistemplate.tex version 3.10 (2018/04/24)                               %%
%% The LaTeX template file to be used with the aaltothesis.sty (version 3.10) %%
%% style file.                                                                %%
%% This package requires pdfx.sty v. 1.5.84 (2017/05/18) or newer.            %%
%%                                                                            %%
%% This is licensed under the terms of the MIT license below.                 %%
%%                                                                            %%
%% Copyright 2017-2018, by Luis R.J. Costa, luis.costa@aalto.fi,              %%
%% Copyright 2017-2018 documentation in Finnish in the template by Perttu     %%
%% Puska, perttu.puska@aalto.fi                                               %%
%% Copyright Swedish translations 2017-2018 by Elisabeth Nyberg,              %%
%% elisabeth.nyberg@aalto.fi and Henrik Wallén, henrik.wallen@aalto.fi        %%
%%                                                                            %%
%% Permission is hereby granted, free of charge, to any person obtaining a    %%
%% copy of this software and associated documentation files (the "Software"), %%
%% to deal in the Software without restriction, including without limitation  %%
%% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
%% and/or sell copies of the Software, and to permit persons to whom the      %%
%% Software is furnished to do so, subject to the following conditions:       %%
%% The above copyright notice and this permission notice shall be included in %%
%% all copies or substantial portions of the Software.                        %%
%% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
%% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
%% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
%% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
%% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
%% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
%% DEALINGS IN THE SOFTWARE.                                                  %%
%%                                                                            %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%%                                                                            %%
%% An example for writting your thesis using LaTeX                            %%
%% Original version and development work by Luis Costa, changes by Perttu     %% 
%% Puska.                                                                     %%
%% Support for Swedish added 15092014                                         %%
%% PDF/A-b support added on 15092017                                          %%
%% PDF/A-2b and PDF/A-3b support added on 24042018                            %%
%%                                                                            %%
%% This example consists of the files                                         %%
%%         thesistemplate.tex (versio 3.10)                                   %%
%%         opinnaytepohja.tex (versio 3.10) (for text in Finnish)             %%
%%         aaltothesis.cls (versio 3.10)                                      %%
%%         kuva1.eps (graphics file)                                          %%
%%         kuva2.eps (graphics file)                                          %%
%%         kuva1.jpg (graphics file)                                          %%
%%         kuva2.jpg (graphics file)                                          %%
%%         kuva1.png (graphics file)                                          %%
%%         kuva2.png (graphics file)                                          %%
%%         kuva1.pdf (graphics file)                                          %%
%%         kuva2.pdf (graphics file)                                          %%
%%                                                                            %%
%%                                                                            %%
%% Typeset in Linux either with                                               %%
%% pdflatex: (recommended method)                                             %%
%%             $ pdflatex thesistemplate                                      %%
%%             $ pdflatex thesistemplate                                      %%
%%                                                                            %%
%%   The result is the file thesistemplate.pdf that is PDF/A compliant, if    %%
%%   you have chosen the proper \documenclass options (see comments below)    %%
%%   and your included graphics files have no problems.
%%                                                                            %%
%% Or                                                                         %%
%% latex:                                                                     %%
%%             $ latex thesistemplate                                         %%
%%             $ latex thesistemplate                                         %%
%%                                                                            %%
%%   The result is the file thesistemplate.dvi, which is converted to ps      %%
%%   format as follows:                                                       %%
%%                                                                            %%
%%             $ dvips thesistemplate -o                                      %%
%%                                                                            %%
%%   and then to pdf as follows:                                              %%
%%                                                                            %%
%%             $ ps2pdf thesistemplate.ps                                     %%
%%                                                                            %%
%%   This pdf file is not PDF/A compliant. You must must make it so using,    %%
%%   e.g., Acrobat Pro or PDF-XChange.                                        %%
%%                                                                            %%
%%                                                                            %%
%% Explanatory comments in this example begin with the characters %%, and     %%
%% changes that the user can make with the character %                        %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% WHAT is PDF/A
%%
%% PDF/A is the ISO-standardized version of the pdf. The standard's goal is to
%% ensure that he file is reproducable even after a long time. PDF/A differs
%% from pdf in that it allows only those pdf features that support long-term
%% archiving of a file. For example, PDF/A requires that all used fonts are
%% embedded in the file, whereas a normal pdf can contain only a link to the
%% fonts in the system of the reader of the file. PDF/A also requires, among
%% other things, data on colour definition and the encryption used.
%% Currently three PDF/A standards exist:
%% PDF/A-1: based on PDF 1.4, standard ISO19005-1, published in 2005.
%%          Includes all the requirements essential for long-term archiving.
%% PDF/A-2: based on PDF 1.7, standard ISO19005-2, published in 2011.
%%          In addition to the above, it supports embedding of OpenType fonts,
%%          transparency in the colour definition and digital signatures.
%% PDF/A-3: based on PDF 1.7, standard ISO19005-3, published in 2012.
%%          Differs from the above only in that it allows embedding of files in
%%          any format (e.g., xml, csv, cad, spreadsheet or wordprocessing
%%          formats) into the pdf file.
%% PDF/A-1 files are not necessarily PDF/A-2 -compatible and PDF/A-2 are not
%% necessarily PDF/A-1 -compatible.
%% All of the above PDF/A standards have two levels:
%% b: (basic) requires that the visual appearance of the document is reliably
%%    reproduceable.
%% a (accessible) in addition to the b-level requirements, specifies how
%%   accessible the pdf file is to assistive software, say, for the physically
%%   impaired.
%% For more details on PDF/A, see, e.g., https://en.wikipedia.org/wiki/PDF/A
%%
%%
%% WHICH PDF/A standard should my thesis conform to?
%%
%% Primarily to the PDF/A-1b standard. All the figures and graphs typically
%% use in thesis work do not require transparency features, a basic '2-D'
%% visualisation suffices. The font to be used are specified in this template
%% and they should not be changed. However, if you have figures where
%% transparency characteristics matter, use the PDF/A-2b standard. Do not use
%% the PDF/A-3b standard for your thesis.
%%
%%
%% WHAT graphics format can I use to produce my PDF/A compliant file?
%%
%% When using pdflatex to compile your work, use jpg, png or pdf files. You may
%% have PDF/A compliance problems with figures in pdf format. Do not use PDF/A
%% compliant graphics files.
%% If you decide to use latex to compile your work, the only acceptable file
%% format for your figure is eps. DO NOT use the ps format for your figures.

%% USE one of these:
%% * the first when using pdflatex, which directly typesets your document in the
%%   chosen pdf/a format and you want to publish your thesis online,

%% * the second when you want to print your thesis to bind it, or
%% * the third when producing a ps file and a pdf/a from it.
%%
\documentclass[english, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, dvips, online]{aaltothesis}

%% Use the following options in the \documentclass macro above:
%% your school: arts, biz, chem, elec, eng, sci
%% the character encoding scheme used by your editor: utf8, latin1
%% thesis language: english, finnish, swedish
%% make an archiveable PDF/A-1b, PDF/A-2b or PDF/A-3b compliant file: a-1b,
%%                    a-2b, a-3b
%%                    (a normal pdf is produced without the a-*b option)
%% typeset in symmetric layout and blue hypertext for online publication: online
%%            (no option is the default, resulting in a wide margin on the
%%             binding side of the page and black hypertext)
%% two-sided printing: twoside (default is one-sided printing)
%%

%% Use one of these if you write in Finnish (see the Finnish template
%% opinnaytepohja.tex)
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, dvips, online]{aaltothesis}

\usepackage{graphicx}
\usepackage{spverbatim}
\usepackage{listings}
\lstset{frame=tb,
  basicstyle={\ttfamily}, 
  language=sh,
  breaklines=true,
  tabsize=4
}

%% Math fonts, symbols, and formatting; these are usually needed
\usepackage{amsfonts,amssymb,amsbsy}

%% Change the school field to specify your school if the automatically set name
%% is wrong
% \university{aalto-yliopisto}
% \school{Sähkötekniikan korkeakoulu}

%% Edit to conform to your degree programme
%%
\degreeprogram{Electronics and electrical engineering}
%%

%% Your major
%%
\major{Communications Engineering}
%%

%% Major subject code
%%
\code{ELEC0007}
%%
 
%% Choose one of the three below
%%
%\univdegree{BSc}
\univdegree{MSc}
%\univdegree{Lic}
%%

%% Your name (self explanatory...)
%%
\thesisauthor{Rohan Krishnakumar}
%%

%% Your thesis title comes here and possibly again together with the Finnish or
%% Swedish abstract. Do not hyphenate the title, and avoid writing too long a
%% title. Should LaTeX typeset a long title unsatisfactorily, you mght have to
%% force a linebreak using the \\ control characters.
%% In this case...
%% Remember, the title should not be hyphenated!
%% A possible "and" in the title should not be the last word in the line, it
%% begins the next line.
%% Specify the title again without the linebreak characters in the optional
%% argument in box brackets. This is done because the title is part of the 
%% metadata in the pdf/a file, and the metadata cannot contain linebreaks.
%%
\thesistitle[Accelerated DPDK in containers for networking nodes]{Accelerated DPDK in containers for networking nodes}

%\thesistitle[Title of the thesis]{Title of\\ the thesis}
%%

%%
\place{Espoo}
%%

%% The date for the bachelor's thesis is the day it is presented
%%
\date{24.4.2018}
%%

%% Thesis supervisor
%% Note the "\" character in the title after the period and before the space
%% and the following character string.
%% This is because the period is not the end of a sentence after which a
%% slightly longer space follows, but what is desired is a regular interword
%% space.
%%
\supervisor{Prof.\ Yu Xiao}
%%
%% Advisor(s)---two at the most---of the thesis.
%%
\advisor{D.Sc.(tech) Vesa Hirvisalo}
\advisor{MSc.\ Kati Ilvonen}
%\advisor{MSc Sarah Scientist}
%%

%% Aaltologo: syntax:
%% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
%% The logo language is set to be the same as the thesis language.
%%
\uselogo{aaltoRed}{''}
%%

%% The English abstract:
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%% Thesis keywords:
%% Note! The keywords are separated using the \spc macro
%%
\keywords{DPDK\spc Containers\spc Docker\spc Kubernetes\spc CNI\spc OvS\spc SR-IOV\spc NFV}
%%

%% The abstract text. This text is included in the metadata of the pdf file as well
%% as the abstract page.
%%
\thesisabstract{
Your abstract in English. Keep the abstract short. The abstract explains your 
research topic, the methods you have used, and the results you obtained. In the 
PDF/A format of this thesis, in addition to the abstract page, the abstract text is 
written into the pdf file's metadata. Write here the text that goes into the 
metadata. The metadata cannot contain special characters, linebreak or paragraph 
break characters, so these must not be used here. If your abstract does not contain 
special characters and it does not require paragraphs, you may take advantage of 
the abstracttext macro (see the comment below). Otherwise, the metadata abstract 
text must be identical to the text on the abstract page.
}

%% Copyright text. Copyright of a work is with the creator/author of the work
%% regardless of whether the copyright mark is explicitly in the work or not.
%% You may, if you wish, publish your work under a Creative Commons license (see
%% creaticecommons.org), in which case the license text must be visible in the
%% work. Write here the copyright text you want. It is written into the metadata
%% of the pdf file as well.
%% Syntax:
%% \copyrigthtext{metadata text}{text visible on the page}
%% 
%% In the macro below, the text written in the metadata must have a \noexpand
%% macro before the \copyright special character, and macros (\copyright and
%% \year here) must be separated by the \ character (space chacter) from the
%% text that follows. The macros in the argument of the \copyrighttext macro
%% automatically insert the year and the author's name. (Note! \ThesisAuthor is
%% an internal macro of the aaltothesis.cls class file).
%% Of course, the same text could have simply been written as
%% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
%% {Copyright \copyright{} 2018 Eddie Engineer}
%%
\copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
{Copyright \copyright{} \number\year{} \ThesisAuthor}

%% You can prevent LaTeX from writing into the xmpdata file (it contains all the 
%% metadata to be written into the pdf file) by setting the writexmpdata switch
%% to 'false'. This allows you to write the metadata in the correct format
%% directly into the file thesistemplate.xmpdata.
%\setboolean{writexmpdatafile}{false}

%% All that is printed on paper starts here
%%
\begin{document}

%% Create the coverpage
%%
\makecoverpage

%% Typeset the copyright text.
%% If you wish, you may leave out the copyright text from the human-readable
%% page of the pdf file. This may seem like a attractive idea for the printed
%% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
%% on the page. However, the recommendation is to print this copyright text.
%%
\makecopyrightpage

%% Note that when writting your thesis in English, place the English abstract
%% first followed by the possible Finnish or Swedish abstract.

%% Abstract text
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%%
\begin{abstractpage}[english]
  Your abstract in English. Keep the abstract short. The abstract explains your
  research topic, the methods you have used, and the results you obtained.  
  
  The abstract text of this thesis is written on the readable abstract page as
  well as into the pdf file's metadata via the $\backslash$thesisabstract macro
  (see above). Write here the text that goes onto the readable abstract page.
  You can have special characters, linebreaks, and paragraphs here. Otherwise,
  this abstract text must be identical to the metadata abstract text.
  
  If your abstract does not contain special characters and it does not require
  paragraphs, you may take advantage of the abstracttext macro (see the comment
  below).
\end{abstractpage}

%% The text in the \thesisabstract macro is stored in the macro \abstractext, so
%% you can use the text metadata abstract directly as follows:
%%
%\begin{abstractpage}[english]
%	\abstracttext{}
%\end{abstractpage}

%% Preface
%%
\mysection{Preface}
I want to thank Professor Yu Xiao for her valuable guidance....

\vspace{5cm}
Otaniemi, 24.4.2018

\vspace{5mm}
{\hfill Rohan Krishnakumar \hspace{1cm}}

%% Force a new page after the preface
%%
\newpage


%% Table of contents. 
%%
\thesistableofcontents


%% Symbols and abbreviations
\mysection{Abbreviations}
\begin{tabular}{ll}
CNI         & Container Networking Interface \\
DPDK        & Data Plane Development Kit \\
k8s         & Kubernetes \\
NFV         & Network Function Virtualization \\
OVN         & Open Virtual Netowrk \\
OvS         & Open vSwitch \\
SR-IOV      & Single Root IO Vitrualization \\
VNF         & Virtual Network Function \\
\end{tabular}


%% \clearpage is similar to \newpage, but it also flushes the floats (figures
%% and tables).
%%
\cleardoublepage

%% Text body begins. Note that since the text body is mostly in Finnish the
%% majority of comments are also in Finnish after this point. There is no point
%% in explaining Finnish-language specific thesis conventions in English.
%% This text will be translated to English soon.
%%
\section{Introduction}

%% Leave page number of the first page empty
%% 
\thispagestyle{empty}
In telecommunication industry, the network node functions have traditionally run on operator proprietary hardware which are dedicated for a specific purpose, known as ASICs (Application Specific Integrated Circuits). This has led to long development and deployment cycles, higher cost of operation, etc., thereby hindering rapid technological advancement in the field. Slowly the network node functions are being replaced by virtualized software solutions that can run on generic computing hardware; consequently addressing the above problems and making the network more flexible. This network architecture where virtualized software solutions are chained together to form network node functions is called Network Function Virtualization (NFV).

In the NFV nomenclature, the software implementation of the network node functions or sub-functions are referred to as Virtual Network Functions (VNF). For instance, a network firewall, a DHCP server or a NAT function. The current de-facto VNF implementations run on virtual machines such as a kernel-based Virtual Machine (KVM). But with the recent advent of containers and their gaining popularity due to some of their obvious advantages over virtual machines, there is a need to explore their suitability in NFV. Containers are comparatively light weight and utilize the same host kernel when compared to virtual machines. They can be booted up with minimum latency\cite{container-vm-comparison} which implies, container based deployments focus on on-demand spawning of the containerized application. For example, a NAT application running on a container could be spawned up based on the number of clients utilizing it, with one container handling one or a fixed number of clients. However, utilizing containers in NFV is still in its nascent stages and there are quite a few issues to be explored and addressed [][].
%% [ https://www.redhat.com/blog/verticalindustries/is-nfv-ready-for-containers/ ], there are several on going studies and  containerization in NFV is still in a nascent stage and  As listed in [ https://www.ietf.org/proceedings/94/slides/slides-94-nfvrg-11.pdf ], some of the key features needed in NFV environment are performance, portability/scalability and isolation. 

The scope of the thesis is to focus on the networking aspect of utilizing containers in NFV. An important requirement in NFV is fast packet processing when compared to normal container based deployments. For a comprehensive understanding of container networking scenario and as a baseline for measurements, the thesis starts with exploring container networking in a standalone environment and then moves on to an orchestrated environment with multiple hosts. The real world use cases mostly rely to an orchestrated environment. The main focus is on incorporating Data Plane Development Kit (DPDK) for fast packet processing and Single Root IO Virtualization (SR-IOV) for hardware virtualization, into the various networking solutions explored.

As depicted in Figure \ref{fig-intro}, a container orchestrated environment could be visualized as a cluster of multiple hosts with containers being spawned on request and killed after serving the request. These containers could be spawned on any of the hosts based on a variety of factors. The orchestrator runs on one host and acts as the master. It normally has a scheduler that decides on which of the other hosts to deploy the container based on the provided requirements of the container and different hardware features supported by different hosts. In the scope of the thesis, the hardware features that are of interest are the support for DPDK and SR-IOV interfaces. 

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm,width=15cm]{pics/ClusterDiag.png}
\end{center}
\caption{A cluster of nodes with one acting as a master. The other nodes can have different hardware features that are supported}
\label{fig-intro}
\end{figure}

The objective of the thesis is to find a fast data plane solution in a container orchestrated environment. There are different scenarios considered based on the availability and features supported by different hosts in the cluster and an intelligent way to deploy the container in the host that meets the latency requirements of the containerized application. To this extend, the topology considers interfaces with DPDK or SR-IOV support that exposed to the external network to some of the hosts in the cluster and also a fast packet forwarding within the cluster to reach the container of interest. In essence, a networking solution that does not affect the flexibility of the cluster and the scheduling decision is done on fly. 

%DPDK utilizes several techniques for better packet processing such as skipping the kernel network stack and running applications on userspace, using poll mode drivers instead interrupt driven ones, hugepages, Direct Memory Access (DMA) and others. Among them, correct utilization of hugepages is an important aspect especially when multiple DPDK based applications are run simultaneously on the same host. Most of the current container run-time applications such as Docker, LXC do not support hugepage isolation and resource limitation. It is possible that one application could end up consuming or pre-allocating all of the available hugepages. Addressing this drawback to support hugepage isolation in containers is another key aspect of this thesis.

Whenever possible, for a better grasp of the performance measurements with DPDK and SR-IOV, a comparative study with and without them is done. For some of the basic tools, only the industry de-facto solutions are considered, such as  Docker for container run-time and Kubernetes for container orchestration. Keeping these constant, the study is on the various networking solutions feasible for them and other aspects described above. The containers used in the experiments run basic DPDK based applications that are shipped with the source code and does a simple forwarding of all packets from one port to another and does not run any application as such. This is a better approach for bench marking latency for a generic scenario.

The next section in the thesis covers the related technologies utilized, their definitions and how they work. This covers the motivation for them and in some cases their evolution. The third chapter covers related work in the field of interest and other miscellaneous information. The forth chapter covers the experiments where various solutions are explored and their latency measurements are bench marked. The fifth chapter puts these measurements together and implements a scheduling algorithm in Kubernetes to pick the best node to meet the latency requirements of the application.

\clearpage
\section{How stuff works}
This section describes the main technologies and tools used in the thesis and how they work.

\subsection{Data Plane Development Kit, DPDK}
DPDK is a "set of libraries and drivers"[1] that enable fast packet processing. It is an open source software framework that can be used to build fast networking applications and is maintained by the Linux Foundation[2]. As compared to a normal kernel network stack, DPDK can provide about twenty five percent [3] improvement in packet processing speed.
%% [1] “DPDK, Data Plane Development Kit”, Retrieved from dpdk.org
%% [2] “Linux Foundation Projects”, Retrieved from https://www.linuxfoundation.org/projects/
%% [3] https://builders.intel.com/university/networkbuilders/course/dpdk-101


The driving factor behind the development of DPDK was to address the handling of extremely fast packet rates, especially in a communication network infrastructure, where the packet sizes are typically smaller and packet rates much higher. The average CPU cycles available for handling a packet is much less when the traditional network stack is used. For instance, a 10 Gigabit Ethernet card receiving packets of size 1024 bytes would receive 1.25 million packets per second. A CPU with 2GHz clock cycles handling these packets will have 1600 cycles per packet. On the other hand, when the packet size is smaller, say 64 bytes, this would be 19.5 million packets per second received and 102 CPU cycles available per packet. The CPU should process every packet with just 102 cycles and this quite a small value.

DPDK utilizes a verity of techniques to address improving the packet processing. Primarily, it uses Poll Mode Drivers (PMD) instead of interrupt driven ones for packet handling. In traditional method, when a packet is received, the CPU does a context switching from the userspace process to kernel. It then runs the Interrupt Service Routine (ISR) and switches back to userspace. This is a big overhead especially for higher packet rates. Poll Mode Drivers run on the userspace and use a dedicated CPU core to handle the traffic on one or multiple interfaces. The CPU continuously polls the kernel network driver and the utilization hits close to hundred percent. The can be checked with \lstinline{top} command in Linux. DPDK uses pthread affinity to disable the kernel scheduler from utilizing those cores.

DPDK utilizes hugepages. Hugepages simply imply bigger page size. The standard Linux page size is 4kB while hugepage size could be 2MB or 1GB. In a normal memory access, the virtual memory address needs to be translated to physical memory before it can be accessed. The process is shown in the figure below. The critical factor affecting performance is the TLB hit. When bigger page sizes are used, there are fewer pages and higher probability of TLB hit.

% >>PIC

Another feature utilized in DPDK is Direct Memory Access. DPDK uses Direct Memory Access to speed up packet buffer copying. Compared to a normal kernel network stack, when a packet is received, DPDK uses DMA to directly lift the buffer to user space. In traditional packet handling in the kernel stack involves copying of buffer multiple times such as, from the NIC buffer to kernel socket buffer (\lstinline{skbuf} in Linux) and from there to the userspace. This reduces performance due to the copying overhead and also due to loss of localization of data leading to fewer cache hits. The method used in DPDK is called zero-copy.

Software prefetching in addition to H/W prefetching. 
Intel DDIO : Available only in some supported network cards. Network packets are directly put into L3 cache instead of being DMA'd into main memory.

Cache alignment : Intel cache lines are 64bytes. Cache alignment increases performance. If data fetched from memory is put in different cache lines, it would require multiple cache fetches to retrieve the data. Alignment, putting in one cache line, decreases cache reads.

Most of the DPDK components run on the userspace avoiding context switching altogether. The packets received on an interface are DMA'd directly to the poll mode drivers. 

Modes of operation of a DPDK application - Run to completion. Each core is assigned a port or a set of ports. The core does the I/O specifically for that port. It accept the incoming packet, process it and send it out through the same port or set of ports. Pipeline model - One core is dedicated to just I/O. All packets received at this port and then sent to different cores using ring buffers. It is more complex and could result in packet order being not maintained.

The main components of DPDK are the core libraries, Poll Mode Drivers for various supported Network Interface Cards (NICs) and other libraries that deal with packet classification, Quality of Service (QoS) and specific to the different platforms [4]. All of these libraries run on the userpace. Also, apart from theses, DPDK uses PCI drivers in the kernel such as IGB-UIO or VFIO-PCI that do basic functionality. When an interface is bound to DPDK, it is detached from the normal kernel driver and attached to one of these DPDK drivers in the kernel.

%% http://doc.dpdk.org/guides-16.04/prog_guide/overview.html

The core libraries in DPDK has an Environment Abstraction Layer (EAL) which hides or abstracts the platform from the libraries and applications running above it. This also deals with memory allocation in huge pages and PCI related buffer handling. DPDK does not allocate memory at run-time but rather pre-allocates memory during initialization. For packet processing, use memory from pool. Return after use. The libraries related to allocating pool of memory (Mempool) consisting of memory buffer (Mbuf) and lockless queues or ring buffers for handling them are part of the core library.

\subsection{Single Root IO Virtualization, SR-IOV}
SR-IOV is the hardware virtualization of a PCI Express (PCIe) device, or specifically, a Network Interface Card, into multiple virtual devices that can be directly assigned to an instance of a virtual machine or a container. SR-IOV has a Physical Function (PF) which acts like a normal PCIe device and multiple Virtual Functions (VF) that are lightweight functions with dedicated Rx/Tx queues[24]. In a virtual machine context, the host machine kernel can be bypassed and the packets can flow directly between the VF and PF.

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm]{pics/SRIOV-VF.png}
\end{center}
\caption{SR-IOV interface with Ethernet Virtual Bridge that sorts packets into one of the Virtual Functions and forwards to the Virtual Machine}
\label{fig-sriov}
\end{figure}

Consider the scenario where one host is running multiple Virtual Machines (VM) and a NIC receiving and transmitting packets for all those VMs. Without any hardware virtualization, when a packet is received by the NIC, it triggers an interrupt to the CPU core that is assigned for handling NIC interrupts. This core services the request and examines the packet. Based on the MAC address or VLAN tag, it forwards the packet to the correct VM by triggering another interrupt on the core that is servicing the virtual machine. This is an overhead since there are multiple interrupt handling in the host even before the packet is transferred to the guest operating system and also there is an extra copy of the packet buffer from the host to the VM. Moreover, when the packet rate is high, one core handling all the packets for all the VMs can be a severe bottle neck. As an enhancement to this, Intel proposed VMDq [1] technology which has separate packet queues for each core. The received packets are put into one of the queues based on the destination MAC or VLAN. Each core services its packet queue by copying the buffer to the VM. There is distribution of work here and a good performance enhancement. An even further enhancement is SR-IOV in which the NIC has separate packet queues for each Virtual Function. When a packet is received, it is sorted to one of the VF queues based on the destination MAC address or VLAN tag by a virtual bridge or classifier. The VF then pushes the packet up the virtual machine directly without involving the CPU using Direct Memory Access (DMA). SR-IOV needs CPU I/O virtualization technology such as Intel VT-d or AMD-Vi enabled for the DMA to the virtual machine. Figure \ref{fig-sriov} depicts the functioning of Virtual Functions in SR-IOV.
%% [1] https://www.intel.com/content/www/us/en/virtualization/vmdq-technology-paper.html

\subsection{Container}
A container is an isolated execution environment which includes any necessary run time dependencies and libraries needed for the application that it packages. It could be thought of as a software that runs on the host with its own slice of the system resources such as network, file systems, user groups, etc. but isolated from other processes. Containers are built on namespaces and control groups which are a part of the Linux kernel.

As per Linux man pages, namespace is an abstraction layer on "global system resources" that provides isolated instance of the resource to the processes that are members of the namespace. Linux provides six different namespaces, Inter Process Communication (IPC) including POSIX and System V message queues, Network, Mount, Process Identifier (PID), User and UTS [1]. With network namespace, the processes in the new namespace can add new network devices without them being available to host network namespace and vice versa. Similarly, with mount namespace, different file systems can be mounted and processes within this namespace cannot access other host file system. Namespace can be created using Linux system call \lstinline{unshare}. The context of namespace that every process belongs to stored in a sub-folder under proc file system as shown below.
%% [1] Linux man paages; man 7 namespaces; man unshare

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ ls -l /proc/$$/ns | awk '{print $1"\t"$(NF-2)$(NF-1)$NF}'
lrwxrwxrwx      cgroup->cgroup:[4026531835]
lrwxrwxrwx      ipc->ipc:[4026531839]
lrwxrwxrwx      mnt->mnt:[4026531840]
lrwxrwxrwx      net->net:[4026532009]
lrwxrwxrwx      pid->pid:[4026531836]
lrwxrwxrwx      pid_for_children->pid:[4026531836]
lrwxrwxrwx      user->user:[4026531837]
lrwxrwxrwx      uts->uts:[4026531838]

\end{lstlisting}

Below is an example of using namespace to create an isolated environment. The current process id in a bash shell can be found out using \lstinline{$$}. Then use \lstinline{unshare} command to create a new namespace. Here the arguments "pid" and "net" signifies new namespaces for process identifier and network. The argument "mount-proc" tells the command to mount the proc file system. The "fork" option makes a fork of the current process before creating the namespace and attaches the child process to it. 

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ echo $$ 
8009
$ unshare --fork --pid --net --mount-proc bash
root:~# ip l
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group
    default qlen 1000 link/loopback 00:00:00:00:00:00
    brd 00:00:00:00:00:00
root:~# ps -aef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 13:37 pts/26   00:00:00 bash
root        12     0  0 13:44 pts/30   00:00:00 -bash
root        32     1  0 13:52 pts/26   00:00:00 ps -aef
root:~#                                  

\end{lstlisting}

The new namespace created for the container is evident from \lstinline{ip link} command as it does not list any of the interfaces available on the host machine. The command \lstinline{nsenter} can used to enter a namespace. The argument \lstinline{-t} takes process identifier of the target process and joins its namespace. The other arguments correspond to the different namespaces that it should enter[].
%[] man nsenter

\begin{lstlisting}[basicstyle={\small\ttfamily}]

$ ps -aef | grep bash | grep root | awk '{print $1"\t"$2"\t"$3"\t"$8}'
root     31215  8009    sudo
root     31294 31215    sudo
root     31295 31294    unshare
root     31296 31295    bash

$ nsenter -m -u -i -n -p  -t 31296
root:/# ps -aef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 13:37 pts/26   00:00:00 bash
root        12     0  0 13:44 pts/30   00:00:00 -bash
root        30    12  0 13:45 pts/30   00:00:00 ps -aef
root:/# 

\end{lstlisting}

As a final step, add a network bridge within the new namespace and it is possible to verify that this bridge is not available in the host namespace.
\begin{lstlisting}[basicstyle={\small\ttfamily}]
root:~# brctl addbr asd
root:~# ip l
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group
    default qlen 1000 link/loopback 00:00:00:00:00:00
    brd 00:00:00:00:00:00
2: asd: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode
    DEFAULT group default qlen 1000 link/ether 4a:18:f1:3c:9c:b2
    brd ff:ff:ff:ff:ff:ff
\end{lstlisting}

Control groups on the other hand are useful in allocating and limiting resource consumption such as memory, CPU time, network bandwidth etc. Control groups or cgroups are organized hierarchically, child cgroups inherit from parent cgroups. The pseudo file system "sys" is used as an interface to create and manipulate cgroups. To create a new cgroup, create a directory under \lstinline{/sys/fs/cgroup} and set appropriate values. For instance, when a Kubernetes pod is created with Hugepage limited to 4GB as described here [], cgroups creates a directory in \lstinline{/sys/fs/cgroup/hugetlb/kubepods/} and sets value of \lstinline{hugetlb.1GB.limit_in_bytes} to 4294967296 (4GB)

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ sudo cat /sys/fs/cgroup/hugetlb/kubepods/hugetlb.1GB.limit_in_bytes 
4294967296

\end{lstlisting}

Any description of containers is incomplete without a comparison to virtual machines. As depicted in Figure \ref{fig-vm-container}, a virtual machine emulates an entire machine. There is a hypervisor that runs on the host machine that emulates the underlying hardware. Then a complete operating system running on top of it. The advantage of virtual machine is that, it is completely isolated from the host machine. The run environment can be guaranteed in spite of the host hardware and operating system. While at the same time, virtual machines can be heavy weight and resource consuming. In contrast, a container essentially runs completely on the host. It uses namespaces and control groups for isolation of the container from the host. As a result, they are much more easy to boot up, lightweight and picking up in popularity when compared to virtual machines [].
%% []

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm,width=15cm]{pics/Container-vm-comparison.png}
\end{center}
\caption{Comaprison between virtual machines and containers}
\label{fig-vm-container}
\end{figure}

\subsubsection{Container Networking Interface}
Container Networking Interface Specification is a proposal for standardized networking solution for container applications. It is documented in GitHub and has its origins from CoreOS and rkt []. According to this specification, container run-time is responsible for creating the network namespace and the network plugin is an executable that called by the container at run-time. It specifies functions that should be exposed by the network plugin such as ADD, DEL, GET and VERSION. For instance, ADD is invoked with the container identifier, network namespace and other network configurations. This triggers the plugin executable to insert an interface into that namespace as per the configuration. Similarly, DEL is invoked when the container is killed.
%% https://github.com/containernetworking/cni/blob/master/SPEC.md

\subsection{Kubernetes}
Kubernetes is an open source container orchestration system. It is used for deploying and managing container applications in a cluster. It is a result of evolution of container management system developed by Google internally, known as Borg. Borg was then replaced by Omega and then finally by Kubernetes[1]. It was made open source in 2014 [2].
% [1] https://queue.acm.org/detail.cfm?id=2898444
% [2] https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/

Typically a Kubernetes cluster consists of at least one master node and many worker nodes. The master node is responsible for maintaining the cluster state, managing the resources, scheduling the work load, etc., while the application containers actually run in the worker nodes. The smallest unit that cab be controlled by Kubernetes is a pod. A pod can have multiple containers running within it but all of them share the same namespace. So, if there are multiple containers that need to interact closely, for instance, through inter process communication (IPC), they can be placed in the same pod. Since, the containers in the same pod will share the same IPC namespace.

The various functionalities of a Kubernetes master node are handled by different components such as, API server, controller manager, etcd and scheduler. These components do not necessarily have to be run on one master node but could be run anywhere in cluster. But as a good practise, to support High Availability (HA), it is encouraged to run these control plane components in one node[].
% [] https://kubernetes.io/docs/setup/independent/high-availability/

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm,width=15cm]{pics/k8s.png}
\end{center}
\caption{Various components Kubernetes master and worker nodes and the way they interact}
\label{fig-k8s}
\end{figure}

The API server exposes API's that are used by other control plane components to retrieve, store and manage information about the cluster. It also acts as an endpoint to communication between the cluster and the Kubernetes master.

The Kubernetes scheduler is responsible for scheduling the workload between the worker nodes. Typically when a pod is requested to be deployed, the scheduler considers the resource requests in the pod and runs  filtering and prioritizing functions and binds the pod to the node with the highest priority. The filtering run filters out nodes that cannot support the pod. For instance, if a pod requests for Hugepages and the nodes that do not support Hugepages are filtered out. The priority function does some load balancing between the worker nodes.

etcd is the database that stores Kubernetes cluster information about. The API server is the only component that interacts with etcd.

Controllers in Kubernetes monitor the state of various resources and try to bring them to the desired state. For instance, the replication controller makes sure the desired number of pods are running in the cluster. If there are too many, the unnecessary ones are killed and if there are fewer, the required number of pods are created. The collection of all such controllers into one application is the controller manager. There is also a cloud controller manager in the Kubernetes master that interacts with the underlying cloud provider.

Kubernetes worker nodes contain kubelet, kube-proxy and the container run-time Docker. Kubelet manages the pods running in a node. It receives the pod configuration from the API server and deploys the pod. It also sends back logs. It is used for connecting to a pod, for instance through the \lstinline{kubectl exec} command.

Kube-proxy handles the networking part of the worker nodes. Implements iptable rules for prot forwarding and exposing service.
% (https://kubernetes.io/docs/concepts/architecture/cloud-controller/)

Components : Master, worker nodes and Controllers, Services, Pods, Namespaces, Network and Storage

k8s master has Kube API server, kube-scheduler and etcd database.
Also a cloud controller manager, other 3rd party softwares, DNS, etc. All needed for a production cluster. Kube API server updates etcd. Only this interacts with etcd.
Kube api server acts like an access point to both "internal and external traffic" and "master process to the entire cluster".
Scheduler decides which node to deploy the pod. Decision is  based on an algorithm. User defined algo could be used or the original one modified. Intel node discovery feature could be used with scheduler to figure out where to deploy.

---- No more detailed writing
k8s worker has kublet and kube-proxy
Kubelet for local configurations

\subsection{Open vSwitch}
Open vSwitch is an open source implementation of a virtual switch which is designed for virtualized environments such as a data center. In essence, it could be considered as a "feature rich" Linux bridge[1].

The main components of OvS are the database server, the main process daemon or\lstinline{ovs-vswitchd} and a data path which is normally a kernel module. Apart from these main components, OvS uses mainly three protocols for communicating between them, viz., OVSDB management protocol[2], OpenFlow and Netlink. The way these components interact along with management tools is depicted in Figure \ref{fig-ovs}
% [1] https://github.com/openvswitch/ovs/blob/master/Documentation/intro/why-ovs.rst
% [2] https://tools.ietf.org/html/draft-pfaff-ovsdb-proto-00

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm, width=15cm]{pics/ovs.png}
\end{center}
\caption{Various components in Open vSwitch separated into management layer, userspace and kernel}
\label{fig-ovs}
\end{figure}

The database server or \lstinline{ovsdb-server} is a persistent storage of OvS configuration. It is configured using an external configuration tool such as the OvS CLI tool \lstinline{ovsdb-tool} or \lstinline{ovs-vsctl}. The former is primer for configuring the database while the latter is for configuring the switch. The database server communicates with \lstinline{ovs-vswitchd} to exchange configuration information using OVSDB management protocol.

The main process daemon, \lstinline{ovs-vswitchd}, is responsible for the managing all OvS bridges in the machine. Network flows are installed using an external controller such as \lstinline{ovs-ofctl}. It then installs these flows in the data path or the kernel module using Netlink.

In this thesis, OvS is configured to run on DPDK. This implies, instead of the kernel module, the forwarding plane resides on the user space. The physical interfaces use Poll Mode Drivers (PMD) running in the user space. 

\clearpage
\section{Related work and Misc}
1. Related work in the field
2. Experimental runs, how many and what is measured
3. Considerations - Packet generators


\clearpage
\section{Latency Measurements}
This section covers the actual experimentation part of the thesis with the motivation to do so. The first two subsections explore a fast data path in container running in a single host. The first of those is a DPDK based container running on a host with SR-IOV interfaces being used for forwarding packets from the NIC to the container and back. The second subsection is a study on Open vSwitch (OvS) and DPDK acceleration. In this case, there is a OvS bridge receiving packets from the NIC and forwarding to the container and back. There is a comparative study to understand how much acceleration is brought about by DPDK.

The remaining subsections consider only a cluster environment with Kubernetes. For a fast data path in this case, multiple scenarios have been considered and some potential solutions discussed and experimented with. For this, initially a Kubernetes cluster is built on bare metal using MAAS (Metal-as-a-Service) and \lstinline{kubeadm} or Juju. Then the different scenarios are outlined and discussed one after the other in a detailed manner.

\subsection{Container on DPDK and SR-IOV}
A default docker container built on a bare Ubuntu image creates by default one interface that is connected to "docker0" bridge on the host. This is the default networking option in Docker [].
%% https://docs.docker.com/network/#scope-of-this-topic

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ docker run -it ubuntu:latest /bin/bash
root@f8344e977497:/# ip l
bash: ip: command not found
root@f8344e977497:/# ifconfig 
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255
        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)
        RX packets 25  bytes 4140 (4.1 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
\end{lstlisting}
The host docker0 bridge in the same sub-net
\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ ip a | grep -A 3 docker0
7: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:29:09:76:49 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:29ff:fe09:7649/64 scope link 
       valid_lft forever preferred_lft forever
\end{lstlisting}

Docker supports other networking options such as host, overlay, macvlan. Host networking option in Docker removes the network isolation between the host and the container. Overlay is an overlay network and can be used to connect to other containers in different host managed by Docker swarm. Macvlan assigns a MAC address to the virtual interface in the container and exposes it to a parent host physical interface making it look like a real physical interface. All of the above options are suited for specific use cases. The final option in Docker networking, using network plugins, is the most interesting one. It is an interface to use third part networking plugins, or CNI plugins. The open source CNI plugins for SR-IOV and DPDK could be used to explore fast data path networking in containers.

The test setup consists of two physical machines. Both the machines have a management interface and two SR-IOV interfaces each. The SR-IOV interfaces are connected in back to back forming a closed loop as show in Figure \ref{fig-test-sriov-dpdk}.
\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm]{pics/ContainerOnDPDKandSRIOV.png}
\end{center}
\caption{Test topology. DPDK based pktgen on one side and a Docker container with DPDK and SR-IOV on the other.}
\label{fig-test-sriov-dpdk}
\end{figure}

One of the machines is used as a packet generator while the other one utilizes virtualization of SR-IOV to push the packet directly to the container. The container also runs a DPDK application that just forwards the packets received on one port to the other. To run DPDK based application, enable SR-IOV Virtual Functions and use CNI plugins with Docker, the host environment has to be configured as described below.

For running DPDK based applications, hugepages should be enabled in the host machine. The hugepage sizes that supported in the most common Linux distributions are 2MB and 1GB. If 1GB hugepages are to be used, it is recommended to have them configured at boot time by setting some kernel boot parameters. This it to ensure the availability of such a large contiguous memory, 1GB, in main memory. This is mostly not possible at run time due to fragmentation. The kernel boot parameters could be set by editing \lstinline{/etc/default/grub} and updating GRUB with \lstinline{update-grub} command. For instance, edit \lstinline{GRUB_CMDLINE_LINUX} to include, \lstinline{GRUB_CMDLINE_LINUX="default_hugepagesz=1G hugepagesz=1G hugepages=4"}. Now update GRUB, reboot machine and check boot parameters and consequently hugepage availability as follows,

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat /proc/cmdline 
BOOT_IMAGE=/boot/vmlinuz-4.4.0-131-generic default_hugepagesz=1G hugepagesz=1G hugepages=4
$ cat /proc/meminfo | grep -i hugepages_
HugePages_Total:       4
HugePages_Free:        4
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:    1048576 kB
\end{lstlisting}

The Hugepages have to be mounted on the host. In some versions of Linux distributions, for instance, in Ubuntu 16.04 and higher, they are mounted automatically in \lstinline{/dev/hugepages}. It is also possible to isolate the CPU cores that will be used for the DPDK application using boot paramters so that the kernel scheduler does not use them. For instance, \lstinline{GRUB_CMDLINE_LINUX="default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2,3,4,5,6,7"}

DPDK supports different Linux kernel drivers such as UIO, VFIO, etc. []. VFIO is used in most of the cases in the thesis since, as stated in DPDK documentation, it is "more robust and secure" []. VFIO kernel module if supported, is shipped with the OS. It also needs IO virtualization such as Intel VT-d and that needs to be enabled in BIOS. IOMMU has to be set as a Kerenel boot parameter,
\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat /proc/cmdline 
BOOT_IMAGE=/boot/vmlinuz-4.4.0-131-generic default_hugepagesz=1G hugepagesz=1G hugepages=4 intel_iommu=on
\end{lstlisting}

Setting up a SR-IOV interface for virtualization requires IOMMU and Intel VT-d support which are also needed when using VFIO driver as noted above. The number of VFs to be enabled can be set using \lstinline{sys} virtual file system in 
\lstinline{/sys/class/net/$IFNAME/device/sriov_numvfs} where \lstinline{$IFNAME} is the name of the SR-IOV interface as displayed in \lstinline{ip link} command. MAC address needs to be specifically set for each VF using \lstinline{ip link set dev $IFNAME vf $i mac aa:bb:cc:dd:ee:01} where \lstinline{$i} is the VF count. The Physical Function uses this to forward a received packet to the correct VF and hence directly to the container or VM it is exposed to.

For using third party CNI plugins, when a Docker container is spawned, it looks for network configuration file in the default location, lstinline{/etc/cni/net.d/} and binary of the plugin in lstinline{/opt/cni/bin/}. The plugin used for this test case is SR-IOV CNI plugin with support for DPDK []
%% https://github.com/intel/sriov-cni

The topology needs a DPDK based container on one side and a packet generator on the other. For building a container that can run a DPDK application, the idea is to start with a base image of some standard Linux distribution such as Ubuntu 16.04, install the required libraries, tools including some debugging tools such as GNU debugger (GDB). DPDK compilation would require Linux header files and other standard build tools, make, etc. Then fetch the DPDK code, compile the libraries and the required application, testpmd. Some extra compile time flags are used to enable debugging with GDB.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ git clone http://dpdk.org/git/dpdk dpdk

$ cat > build-dpdk.sh<<EOF
echo "Build DPDK"
make install T="\$RTE_TARGET" EXTRA_CFLAGS="-g -O0" -j8
if [ \$? != 0 ]; then
    echo -e "\tBuilding DPDK failed"
    exit 1
fi
cd app/test-pmd/
make -j8
EOF

$ chmod +x build-dpdk.sh
$ cat > Dockerfile <<EOF
FROM ubuntu:16.04
RUN apt-get update; apt-get install -y apt-utils build-essential less make kmod vim pciutils libnuma-dev python gdb linux-headers-$(uname -r);
COPY ./dpdk /root/dpdk
COPY ./build-dpdk.sh /root/dpdk/build-dpdk.sh
WORKDIR /root/dpdk/
ENV RTE_SDK "/root/dpdk"
ENV RTE_TARGET "x86_64-native-linuxapp-gcc"
RUN ./build-dpdk.sh
EOF

$ docker build -t dpdk-docker:latest .
\end{lstlisting}

Now, as is visible from Fig.[], one VF per SR-IOV interface is sufficient for this experiment. However, one VF from each SR-IOV interface is exposed to the container. The CNI plugin configuration used is as follows,
\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat /etc/cni/net.d/10-sriov-dpdk.conf
{
    "name": "net1",
    "type": "sriov",
    "if0": "enp1s0f1",
    "if0name": "if0",
    "dpdk": {
        "kernel_driver":"ixgbevf",
        "dpdk_driver":"igb_uio",
        "dpdk_tool":"\$RTE_SDK/usertools/dpdk-devbind.py"
    }
}
\end{lstlisting}

In the configuration above, \lstinline{type} is always \lstinline{sriov} for this plugin, \lstinline{if0} is the name of the SR-IOV interface or PF, \lstinline{if0name} is the name inside the container and within dpdk parameters, \lstinline{kernel_driver} is the current kernel driver the interface is using, \lstinline{dpdk_driver} is the DPDK driver that needs to be used, such as VFIO and \lstinline{dpdk_tool} is a tool that is shipped with DPDK source code that can be used to binding interfaces to DPDK. The above configuration could be replicated with different \lstinline{if0} and \lstinline{if0name} for the VF from second SR-IOV interface.
%% [] https://github.com/intel/sriov-cni

To run DPDK application in the container, the container has to be run in privileged mode so that it bind the interfaces, get hugepage information, etc. This can be done with the \lstinline{--privileged} flag in Docker. Also, the hugepage mount location has to be mounted in the container as a volumn. Docker run command is as follows,
\begin{lstlisting}[basicstyle={\small\ttfamily}]
docker run -it -v /dev/hugepages:/dev/hugepages --privileged dpdk-docker:gdb /bin/bash
\end{lstlisting}

Inside the container, start testpmd application and configure it for forwarding packets received on one port to the other. If there are an even number of ports attached to DPDK, this is also the default behavior. This can be checked with \lstinline{show config fwd} command inside testpmd. Start Rx/Tx with \lstinline{start} command.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
testpmd> show config fwd
io packet forwarding - ports=2 - cores=1 - streams=2 - NUMA support enabled, MP over anonymous pages disabled
Logical Core 1 (socket 0) forwards packets on 2 streams:
  RX P=0/Q=0 (socket 0) -> TX P=1/Q=0 (socket 0) peer=02:00:00:00:00:01
  RX P=1/Q=0 (socket 0) -> TX P=0/Q=0 (socket 0) peer=02:00:00:00:00:00

testpmd> start
\end{lstlisting}

On the packet generator side, DPDK based packet generator "pktgen" is used. Since this is a DPDK application, similar steps as above are repeated. Bind both interfaces to DPDK and start pktgen.
Latency has to be specifically enabled in DPDK interfaces in pktgen and the packet size set to 96 so that there is room in the buffer for time stamping. Here, we set the destination MAC address as the MAC address of the receiving VF, otherwise the packets get dropped at PF.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
./app/x86_64-native-linuxapp-gcc/pktgen -l 0-3 -n 4 --proc-type auto --log-level 7 --socket-mem 2048 --file-prefix pg -- -T -P --crc-strip -m [1:2].0 -m [2:3].1 -f themes/black-yellow.theme
pktgen> enable 1 latency
pktgen> set 1 dst mac aa:bb:cc:dd:ee:00
pktgen> set 1 size 96
pktgen> start 1
\end{lstlisting}
Above, 1 refers to the port number used for generating packets. And with \lstinline{start 1}, packets are generated at approximately 11 million packets per second (Mpps), as reported by pktgen.

The latency was about 800 micro seconds.

For a comparative study, instead of the host with DPDK based container and packet forwarded through a DPDK based application (testpmd), let the packet packet processing happen via. the standard Linux kernel stack. For this, create a Linux bridge and add the two interfaces to it.

The latency was about 8700 micro seconds for a packet rate of 10Mpps.

\subsection{Container on OVS-DPDK}
The test setup in the previous section used was probably the simplest network possible. The machines were directly connected and the traffic blindly forwarded from one port to another. In a more realistic scenario, there could be multiple containers running on the host and packets forwarded between them. In a container orchestrated environment with Kubernetes, these containers or pods could very well reside in different hosts. This calls for exploring a more generic networking solution.

The default networking provided with Docker, using \lstinline{docker0} bridge is a good solution that would work for containers in one host. Similarly, the overlay network driver in Docker and other third part network plugins such as using Flannel[], Weave Net[2], Calico[3], etc. which also work with Kubernetes are also solutions that could work with little hassle. But none of these methods provide support for a fast data path using DPDK or support virtualization with SR-IOV VFs. However, Open vSwitch is one plausible solution. It supports DPDK and provides a way to easily configure the network using Open Flow. Though it might be an overhead in a simple topology, it could scale well when the network is more complicated with multiple hosts and different types of interfaces such as for management and data since it also supports setting up tunneling interfaces to configure an overlay network which could be helpful with Kubernetes. However, whether the overlay network can support a DPDK based fast data path is still to be explored.
%% [1] https://github.com/coreos/flannel
%% [2] https://github.com/weaveworks/weave
%% [3] https://docs.projectcalico.org/v2.0/getting-started/kubernetes/

This section uses OvS with DPDK and a container with DPDK application, testpmd on one side. Both the SR-IOV interfaces are this time connected to the OvS bridge. The other packet generator side is unchanged as the previous test scenario.

OvS releases page[1] notes the compatible versions of DPDK with OvS and also the compatible version of OvS for different Linux kernel versions. In this experiment, the Linux kernel version used is \lstinline{4.4.0-131-generic}. Correspondingly, the latest compatible version of OvS is 2.9.x and DPDK version compatible this version of OvS is 17.11.3
%% [1] http://docs.openvswitch.org/en/latest/faq/releases/

To use Open vSwitch (OvS) with DPDK data path, DPDK needs to be built as in normal scenario but with the correct version as described above. The environment variables \lstinline{RTE_SDK} and \lstinline{RTE_TARGET} should be set. The former is the path to DPDK source code and the later is the build target environment, or \lstinline{x86_64-native-linuxapp-gcc} in this case. The OvS needs to be configured with \lstinline{--with-dpdk="$RTE_SDK/$RTE_TARGET"} and compiled.

DPDK parameters such as lcore, memory and other typical settings used with most other DPDK applications can be set in the OvS data base as shown below,

\begin{lstlisting}[basicstyle={\small\ttfamily}]
sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true
sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x0F
sudo ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=256
sudo ovs-vsctl --no-wait set Open_vSwitch . external-ids:system-id="${HOSTNAME}id"

\end{lstlisting}

The system ID could come in handy when setting up an overlay network with OvS. Most other steps in setting up OvS with DPDK are quite straightforward or well documented and not listed here.

The final part of the previous section had a Linux bridge with interfaces attached to it and packets forwarded as a normal bridge would do, flood. In this test, something similar in theory is done with an OvS bridge. But, the interfaces are bound to DPDK and then attached to an OvS bridge. Also couple of flows are added to the bridge to forward the packets received on one interface to the other and vice versa instead of flooding. The commands are shown in detail below.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ ovs-vsctl add-br dpdk-br -- set bridge dpdk-br datapath_type=netdev
$ ovs-vsctl add-port dpdk-br p1 -- set Interface p1 type=dpdk options:dpdk-devargs=0000:01:00.0 ofport_request=1
$ ovs-vsctl add-port dpdk-br p2 -- set Interface p2 type=dpdk options:dpdk-devargs=0000:01:00.1 ofport_request=2
$ ovs-ofctl add-flow ext1 in_port=2,action=output:1
$ ovs-ofctl add-flow ext1 in_port=1,action=output:2
\end{lstlisting}

Now from the other end, packets were generated at an average of 10Mpps and the latency was observed to be around 800 micro seconds. This is the same value of latency observed in the previous section with SR-IOV VF exposed to a container and a DPDK based application running on a container. This shows that OvS running as a DPDK application does not add any additional latency. Also, counter intuitively, a container running DPDK application gives same latency as OvS on DPDK. This implies, a container as such does not add any latency to slow down the processing of packets. Which it should not theoretically as well.

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm]{pics/Container-ovs-dpdk.png}
\end{center}
\caption{Test topology with OvS DPDK running on userspace and forwarding packets to a container using vhostuser ports}
\label{fig-test-ovs-dpdk}
\end{figure}

Now, the next logical step is to add a container in between the two interfaces and forward the packets to a DPDK application, testpmd, running inside the container and forward the packets back to the packet generator as shown in Figure \ref{fig-test-ovs-dpdk}. To expose DPDK interfaces to a container via. OvS, OvS uses \lstinline{vhostuser} port on the host side. This creates a UNIX domain socket which needs to be exposed to the container. In DPDK application, a \lstinline{virtiouser} port is created using this socket file. Now, flows are added to forward packets between the DPDK interface and \lstinline{vhostuser} ports. The packets received at \lstinline{vhostuser} will be picked by \lstinline{virtiouser} and thus the DPDK application.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ ovs-vsctl add-port dpdk-br vhu-p1 -- set Interface vhu-p0 type=dpdkvhostuser
$ ovs-vsctl add-port dpdk-br vhu-p2 -- set Interface vhu-p1 type=dpdkvhostuser
$ ovs-ofctl add-flow dpdk-br in_port=1,actions=output:3
$ ovs-ofctl add-flow dpdk-br in_port=4,actions=output:2
\end{lstlisting}

The port numbers in OvS could be verified with command \lstinline{ovs-ofctl dump-ports-desc dpdk-br}. The UNIX domain sockets are created by default in location \lstinline{/usr/local/var/run/openvswitch/}. So, this also has to be mounted while running DPDK Docker apart from the hugepages mount.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
docker run --privileged -it -v /dev/hugepages:/dev/hugepages -v /usr/local/var/run/openvswitch:/var/run dpdk-docker:gdb /bin/bash
\end{lstlisting}

While running the DPDK application inside container, the PCI devices are disabled so that the application does not look for physical ports and virtual devices are created using the two \lstinline{virtiouser} ports.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
./testpmd -c 0xf -n 4 -m 1024 --no-pci --vdev=virtio_user0,path=/var/run/vhu-p1 --vdev=virtio_user1,path=/var/run/vhu-p2 --file-prefix=container -- -i
\end{lstlisting}

Now, as before, the other host starts sending packets at 11Mpps and the latency was observed to be approximately 5000 micro seconds.

\subsection{Fast packet processing in Kubernetes}
The previous two sections dealt with the networking aspects when containers were hosted on just one  host and packets were generated from another. But in a real scenario, there would be a cluster of many hosts with some container orchestration such as Kubernetes managing them. When a request to deploy a new container or rather a pod, in Kubernetes is received, Kubernetes decides on which worker node to deploy depending on the requirements to run the pod such as available memory, CPU cores and any other constraints that may be provided in the pod configuration. Kubernetes has to find a node that fits all the bills. Fast data path could be also be another constraint to be considered while deploying pods. 

In a Kubernetes deployed pod, there has to be one management interface to communicate with Kubernetes, which runs on all the worker nodes. Then, for fast packet processing, some other pod interfaces that are connected to the DPDK bound physical interfaces. These DPDK interfaces could be exposed to the external network. So, when a pod is being deployed in Kubernetes and if it needs external DPDK interfaces, Kubernetes has to decide on which node to deploy the pod. There are three scenarios to be considered and are described below. Also, note that "external" in external DPDK interfaces only imply they are not managed by Kubernetes.

The first scenario is when the required number of DPDK interfaces for the pod is present in all of the Kubernetes worker nodes. In this case the pod could be deployed in any of the nodes and there is no extra networking constraint to be considered by the scheduler. The only question to be answered here is how to expose multiple interfaces to a pod since a Kubernetes pod in its default state supports only one interface.

The second scenario is when the required external DPDK interaces for the pod are present in just some of the worker nodes. In this case, the Kubernetes scheduler needs some mechanism to find out which nodes satisfy this criteria and deploy the pod in one of them.

The third scenario is the trickiest. If the required number of DPDK interfaces are say, three, and there are three worker nodes with one DPDK interface each. The decision on which node to deploy the pod is not important but rather routing the data between the worker nodes. Earlier it was noted that every Kubernetes pod has a default management interface. This could maybe leveraged to route the data packets as well.
%% [] https://kubernetes.io/docs/concepts/architecture/master-node-communication/

The above three scenarios and their potential solutions are discussed in the next few sections. But the next immediate section is on how to build a Kubernetes cluster on bare metal.

\subsection{Building a Kubernetes cluster}
In this section, using bare metal machines, a cluster is formed with local connectivity, a DHCP server to assign dynamic private IP, and a DNS resolver. Then Kubernets is run on this cluster. To build the cluster, MAAS or Metal as a Service, is used. MAAS is an open source software developed by Canonical[]. It helps form a cluster from set of bare metal machines with cloud like features and flexibility.
%% [] https://www.canonical.com/

MAAS needs a central machine which is the controller. The controller is responsible adding further machines to the cluster, assigning IP address, PXE booting them, etc. The other nodes are generally accessible through the controller. The topology is shown in Figure \ref{fig-maas-topo}. The easiest way to setup the MAAS controller is using a Ubuntu server image and in the install options, choosing MAAS region controller. Another option is to install standard Ubuntu distribution, use the package manager, aptitude and install MAAS. The next step is to configure the controller. Most of the configurations are done using the HTTP server that runs by default on port 5240 on the controller. A MAAS cluster can have two types of controllers, region and rack controller. Since the setup is quite small and the machines are physically attached to the same sub network, just one controller is used which acts both as region and rack controller. 

\begin{figure}[htb]
\begin{center}
\includegraphics[height=8cm]{pics/maas-topo.png}
\end{center}
\caption{Topology of cluster built with MAAS}
\label{fig-maas-topo}
\end{figure}

Initially, the controller SSH public key will have to be provided to MAAS. This is later used for allowing access for the controller to the other nodes added to the cluster. Next, the version of Ubuntu to be used for deploying the nodes was set to 16.04. For upstream name resolution, the upstream DNS configuration can be set. Also, since DPDK based applications are run Kubernetes and some of the non-management interfaces in the some of the nodes are SR-IOV, the required Kernel boot parameters can be set. All of these setting are done in the settings page in MAAS controller. 
\begin{lstlisting}[basicstyle={\small\ttfamily}]
intel_iommu=on iommu=pt default_hugepagesz=1G hugepagesz=1G hugepages=4
\end{lstlisting}

To allow access to the external network for other nodes in the cluster, the controller will have be configured to use NAT to forward the traffic. For convenience, let the interface connected to the external network in the controller be \lstinline{eth0} and the interface connected to the subnet \lstinline{10.10.10.0/24} be \lstinline{eth1}. The \lstinline{iptable} configurations for NAT'in are as shown below.
\begin{lstlisting}[basicstyle={\small\ttfamily}]
iptables --table nat --append POSTROUTING --out-interface eth0 -j MASQUERADE
iptables --append FORWARD --in-interface eth1 -j ACCEPT
echo 1 > /proc/sys/net/ipv4/ip_forward
\end{lstlisting}

Other nodes to be added to the cluster should be placed in the sub-net. A switch could be connected to the MAAS controller interface \lstinline{eth1}. Other nodes should be connected to the switch using the interface with PXE boot enabled. On booting, the node gets a temporary IP address from the DHCP server running on the controller. The controller then lists the node in the MAAS dashboard. The power type in the node has to be configured. If the node supports IPMI, it will detected automatically. Otherwise, set the power type to "Manual". The disadvantage is that, the machines will have to be powered on/off manually. Now, in the MAAS dashboard, it is possible to commission the node. The machine will boot again, or will have to be done so manually, after which the controller sends the boot images and initialization scripts. The machine finally goes to ready state.

Once all the nodes are added and in "Ready" state, they can be acquired while deploying some application in MAAS. The application of interest here is Kubernetes and the easiest way to deploy Kubernetes on MAAS is using Juju, as documented in[]. A Kubernetes deployment typically has many components that need to be installed. For instance, in the master node, "etcd", "easyrsa" for generating certificates for TLS, kubectl application, some network plugin and of course the core Kubernetes applications such as scheduler, config manager, etc. On the worker nodes, Docker, kublet, the proxy daemon, etc. Juju does a automatic deployment of all the necessary components. But for this, Juju needs to be deployed first and it needs a dedicated controller. One way to do so without consuming an additional machine is to use a virtual machine in MAAS controller and add it to the cluster. It could then be used it as Juju controller. The other way is to  use any other node in MAAS cluster as Juju controller. The choice depends on how machines are available and how many are needed for Kuberentes. In this experiment, the virtual machine method was used initially. Juju can be installed on MAAS with Juju can deploy a minimal Kubernetes cluster with two machines using the command, \lstinline{juju deploy cs:bundle/kubernetes-core-251}. Verify the status of machines go to "active" with the following command and sample output

\begin{lstlisting}[basicstyle={\tiny\ttfamily}]

ubuntu@labmpd3:~$ juju status
Model    Controller  Cloud/Region  Version  SLA
default  test-cloud  test-cloud    2.3.8    unsupported 

App                Version  Status  Scale  Charm              Store       Rev  OS      Notes
easyrsa            3.0.1    active      1  easyrsa            jujucharms   27  ubuntu  
etcd               2.3.8    active      1  etcd               jujucharms   63  ubuntu  
flannel            0.9.1    active      2  flannel            jujucharms   40  ubuntu  
kubernetes-master  1.9.8    active      1  kubernetes-master  jujucharms   78  ubuntu  exposed
kubernetes-worker  1.9.8    active      1  kubernetes-worker  jujucharms   81  ubuntu  exposed 

Unit                  Workload  Agent  Machine  Public address  Ports           Message
easyrsa/0*            active    idle   0/lxd/0  10.10.10.8                      Certificate Authority connected.
etcd/0*               active    idle   0        10.10.10.6      2379/tcp        Healthy with 1 known peer
kubernetes-master/0*  active    idle   0        10.10.10.6      6443/tcp        Kubernetes master running.
  flannel/0*          active    idle            10.10.10.6                      Flannel subnet 10.1.18.1/24
kubernetes-worker/0*  active    idle   1        10.10.10.7      80/tcp,443/tcp  Kubernetes worker running.
  flannel/1           active    idle            10.10.10.7                      Flannel subnet 10.1.34.1/24

Machine  State    DNS         Inst id              Series  AZ       Message

0        started  10.10.10.6  yda8rf               xenial  default  Deployed

0/lxd/0  started  10.10.10.8  juju-90b946-0-lxd-0  xenial  default  Container started

1        started  10.10.10.7  6hn4pg               xenial  default  Deployed
\end{lstlisting}

Following is a hello world example to test the Kubernetes deployment. To deploy a simple pod in Kubernetes, create pod manifest file and use \lstinline{kubectl} commands to create the pod.

\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat > helloworld.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: ubuntu:latest
    command: ["/bin/bash"]
    args: ["-c", "echo SUCCESS; while true; do sleep 1; done"]
EOF
$ kubectl create -f helloworld.yaml
$ kubectl get pods
NAME            READY     STATUS        RESTARTS   AGE
test-pod        1/1       Running       0          5s
$ kubectl logs test-pod
SUCCESS
\end{lstlisting}

Another way to deploy Kubernetes is using the tool \lstinline{kubeadm}. This is a Kubernetes project that is still in beta stage but quite good for development purposes. This deployment of Kubernets is quite minimalistic while comapred to deploying with Juju. This gives a bit more control. With \lstinline{kubeadm}, the network plugin has to be installed separately which is ideal for most of the work in the thesis since, it can be easily tweaked and modified.

\subsection{Scenario 1, Multus}
This section covers in detail the first scenario described above where all the worker nodes in the Kubernetes cluster have DPDK supported interfaces. The scheduling decision is hence unaffected by the worker node interface hardware features and the normal scheduling decision could be made based on the CPU, memory  and any other resource availability.

To use the DPDK interfaces in a pod, a Container Network Interface (CNI) plugin called "Multus" can be used. It is an open source and developed by Intel[] and written in Golang. Multus acts as an interface between Kubernetes and other CNI plugins. It reads the required networking requirement from a configuration file which specifies which other CNI plugins to use along with their individual configuration. It then looks for these CNI plugin executable in a specific location and runs them with the given configuration. Consequently exposing the CNI plugin based interfaces to the pod. Multus also maintains one interface for Kubernetes related communication. This is called the master plugin. In essence, if CNI plugin, say P1 needs to be called with configuration C1 and another CNI plugin P2 with configuration C2, the user gives these configuration to Multus and Multus calls these individual CNI plugins one after the other exposing it to the pod.
% https://github.com/intel/multus-cni

The SR-IOV with DPDK CNI Plugin used in section 4.1 and Flannel can be used with Multus for this case. Here Flannel is the master plugin used for Kubernetes management interfaces and SR-IOV DPDK plugin is for fast packet processing and latency measurements.
% https://github.com/intel/sriov-cni

There are couple of different ways of using Multus. Kubernetes allows users to define custom resource types, called Custom Resource Definition (CRD). The first approach involves defining a new network resource as a CRD and specifying the plugin configurations while defining the resource objects. The other option is to have a CNI configuration file in each of the worker node with the required configuration. The second method has a disadvantage that all the pods running in a worker will use the same configuration file and it is not possible to customize the network per pod. Since a generic solution is really not needed for the targeted latency measurements, the second approach is used. Multus configuration file is below.

\begin{lstlisting}[basicstyle={\small\ttfamily}]

$ cat /etc/cni/net.d/10-multus.conf
{
    "name": "multus-demo-network",
    "type": "multus",
    "delegates": [
       {
            "type": "flannel",
            "masterplugin": true,
            "delegate": {
                "isDefaultGateway": true
            }
        },
        {
            "type": "sriov",
            "name": "net1",
            "if0": "enp1s0f1",
            "if0name": "if0",
            "dpdk": {
                "kernel_driver":"ixgbevf",
                "dpdk_driver":"igb_uio",
                "dpdk_tool":"\$RTE_SDK/usertools/dpdk-devbind.py"
            }
        },
        {
            "type": "sriov",
            "name": "net2",
            "if0": "enp1s0f2",
            "if0name": "if1",
            "dpdk": {
                "kernel_driver":"ixgbevf",
                "dpdk_driver":"igb_uio",
                "dpdk_tool":"\$RTE_SDK/usertools/dpdk-devbind.py"
            }
        }
    ]
}

\end{lstlisting}

The experiment in this section is similar to the previous sections with a packet generator on one side sending and receiving packets and calculating the latency. But this time on the other side is a Kubernetes cluster with the SR-IOV interfaces from the packet generator directly connected to one of worker node. The SR-IOV VFs are exposed to the Kubernets pod using Multus and other CNI plugins. Here the idea is to measure the latency when all the Kubernets worker nodes have DPDK interfaces. The experiment is quite similar to the one in Section 4.1 but instead of a Docker container, a Kubernets pod is used and Multus is used for multiple interfaces.

The latency measurements were found to be around 800 micro seconds, similar to the values in Section 4.1. This is quite expected since pod and Multus do not really cause any overhead.

\subsection{Scenario 2, Node feature discovery}
In this scenario, only some of the Kubernetes worker nodes may have the required number of DPDK interfaces and the pod needs to be deployed in such a node. To achieve this, Kubernetes scheduler should be aware of nodes that support DPDK or SR-IOV interfaces. There is a Kuberentes incubator project called "Node Feature Discovery" [] that does exactly this, discovering different hardware features supported by the worker nodes.






\subsection{Scenario 3, Kubernetes with OVN}
Exposing external interface and routing within
Big section



%% http://docs.openvswitch.org/en/latest/howto/userspace-tunneling/

\clearpage
\section{Latency based scheduling}
The previous section explored various fast data path solution for a container in a stand alone and also in a cluster environment. Based on the results, it is clear that some of the solutions provide much better latency results. The latency introduced in some of the solutions were software induced and in some others, due to the extra hops between the nodes. When DPDK is used in the native mode, the latency was found to be about 800 micro seconds. With the addition of OvS running on DPDK in between, the latency shot up to 5000 micro seconds. The reason was clearly observed to be the addition of \lstinline{vhostuser} ports to connect the DPDK interfaces to the container. This was a six fold increase but still a better result than using the normal kernel stack. In the Kubernetes scenario, with Multus, the latency there was not affected. This is understandable since Multus calls all CNI plugins configured for the container and attaches the interfaces directly to the container not interfering and adding extra softwre overhead. Hence, a reading of 800 micro seconds on average was observed. The other two solutions explored for Kubernetes were using OvS and OvN. The former was similar to OvS standalone scenario but with the knowledge of the hosts in the cluster with DPDK supported interface. This eventually turns out to be the previous scenario and similar readings were observed. The final scenario with OVN configured a network within the cluster to route data packets between hosts. The overhead here was due to the extra hops between hosts and the interfaces that do not support DPDK.

This section is an attempt to utilize these results while deploying containers in a cluster so that their latency requirements can be met. The idea is that depending on the latency requirement given at the time of creating a container in a cluster, the right host is chosen. For instance, if the latency requirement could be met with OvS on DPDK and with Multus, OvS is chosen since Multus provides lower latency and could be used for containers with more stringent latency requirements. Again, Kubernetes is used as the container orchestration and Docker as the run-time.

\subsection{A Kubernetes Scheduler}
The default scheduler in Kubernetes considers only CPU and memory resources in the worker nodes while scheduling pods[1]. This implies, a new method to provide the latency requirement for a pod has to be found and also a scheduler capable of implementing this.

Annotations in Kubernetes could be used to specify the latency requirements for a pod during deployment. As per the Kubernetes documentation [2], annotations are used for specifying meta-data and can be retrieved using the client libraries. For this purpose, the latency requirement is given as a \lstinline{key} and \lstinline{value} pair.
% [1] https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-types
% [2] https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
\begin{lstlisting}[basicstyle={\small\ttfamily}]
  annotations:
    latency : "5000"
\end{lstlisting}

In Kubernetes scheduler, there are multiple steps involved while deciding which node to bind the pod. In the first step, the scheduler filters out the incompatible nodes. For instance, nodes that do not have enough CPU or memory. Then, from the compatible nodes, the scheduler assigns priority to them while trying to distribute the work load among all the worker nodes. In the last step, the scheduler binds the pod to the node with the highest priority and informs the API server [].
% https://github.com/fabric8io/kansible/blob/master/vendor/k8s.io/kubernetes/docs/devel/scheduler.md

There are different ways to affect the scheduling decision in Kubernetes. One way is to modify the Kubernetes source code and implement the required features. The next method is to implement a Kubernetes scheduler extender. A scheduler extender runs on an IP and port that is accessible to the default scheduler and is called once the default scheduler is done with the first two steps mentioned above, filtering and prioritizing. This is like an extra step in the scheduling process. The final approach is to write a new scheduler that replaces the existing default scheduler. This could also be run as a pod in the cluster itself.

The source code scheduler implemented for the thesis is listed in Appendix \ref{LiiteA}. The scheduler communicates with the Kubernetes API server using the \lstinline{kubeconfig} file or it could be run as a pod in the cluster. It initially gets a list of nodes in the cluster and pods that are configured to use scheduler with name \lstinline{latency-scheduler}. It then runs the scheduling algorithm, finds the node that satisfies the latency requirement and sends a Bind message to the API server to bind the pod to that node. From a pod perspective, to use this scheduler, the scheduler name and latency requirements have to be specified in the manifest.

The second important factor to be addressed for the scheduler to work is to discover the hardware features of the worker nodes; specifically the support for DPDK and SR-IOV interfaces. For this, a daemon set is used[]. A Daemon set runs on all the nodes in Kubernetes. This could be utilized to fetch the interface hardware information and report to the API server. The code to implement this daemon set is listed in Appendix \ref{LiiteB} along with the daemon set manifest file and the Dockerfile to build a Golang container with the application. The code currently adds a new \lstinline{map} entry of \lstinline{dpdk:<number-of-dpdk-interfaces>} to \lstinline{node.Labels} [2] where \lstinline{node} is an object of the \lstinline{Node} structure in Kubernetes Core v1. This information is in turn used by the scheduler.
% https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
% https://godoc.org/k8s.io/api/core/v1#Node

\clearpage
\section{Future Work}
1. Optimizing DPDK applications in general and in Kubernetes - NUMA
2. Hugepage isolation
\clearpage
\thesisbibliography
\begin{thebibliography}{99}

\bibitem{container-vm-comparison} Joy,\ M.,\ A., Performance Comparison Between Linux Containers and Virtual Machines. \textit{International Conference on Advances in Computer Engineering and Applications.} IMS Engineering College, Ghaziabad, India, 2015
\end{thebibliography}

%% Appendices
%% If you don't have appendices, remove \clearpage and \thesisappendix below.
\clearpage
\thesisappendix
\section{Appendix A\label{LiiteA}}
Code written in Golang to implement the basic Kubernetes scheduler along with a Pod manifest to chose this scheduler.
\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat main.go
package main

import (
    "flag"
    "fmt"
    apiv1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/clientcmd"
    "log"
    "os"
)

func main() {
    log.Println("Latency scheduler")

    // Parse arguments
    var SchedulerName string
    flag.StringVar(&SchedulerName, "scheduler-name", "latency-scheduler",
            "Name for the k8s scheduler")

    defaultKubeConfig := os.Getenv("HOME") + "/.kube/config"
    var ns, label, field, kubeconfig string
    
    flag.StringVar(&ns, "namespace", "default", "namespace")
    flag.StringVar(&label, "label", "", "Label selector")
    flag.StringVar(&field, "field", "", "Field selector")
    flag.StringVar(&kubeconfig, "kubeconfig", defaultKubeConfig,
                "Absoulte path to kubeconfig")
    var runInCluster = flag.Bool("run-in-cluster", false, "Run in cluster as a pod?")
    var timeout = flag.Int64("timeout", 5, "Timeout for watching for new pods")

    flag.Parse()

    // Get clientset. If running this process as a pod in the cluster,
    // remove both args in func BuildConfigFromFlags
    var config *rest.Config
    var err error
    if *runInCluster {
            config, err = rest.InClusterConfig()
    } else {
            config, err = clientcmd.BuildConfigFromFlags("", kubeconfig)
    }
    if err != nil {
            log.Fatal(err)
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
            log.Fatal(err)
    }

    corev1 := clientset.CoreV1()

    listOptions := metav1.ListOptions{
            LabelSelector: label,
            FieldSelector: field,
    }

    nodes, err := corev1.Nodes().List(listOptions)
    if err != nil {
            log.Fatal(err)
    }

    watchListOptions := metav1.ListOptions{
            Watch:          true,
            TimeoutSeconds: timeout,
            LabelSelector:  label,
            FieldSelector:  field,
    }
    podWatcher, err := corev1.Pods(ns).Watch(watchListOptions)
    if err != nil {
            log.Fatal(err)
    }

    go func() {
            for {
                  event := <-podWatcher.ResultChan()
                    pod, ok := event.Object.(*apiv1.Pod)
                    if !ok {
                            log.Fatal("Failed retreiving pod")
                    }
                    if pod.Status.Phase == "Pending" && pod.Spec.SchedulerName == SchedulerName {
                            SchedulePod(*pod, *nodes, clientset)
                    }
            }
    }()
    fmt.Scanln()
    log.Println("Done")
}

$ cat scheduler.go
package main

import (
    "errors"
    apiv1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "log"
    "strconv"
)

const (
    LATENCY_WITH_HOPS   = 5000
    LATENCY_WITH_MULTUS = 800
    LATENCY_DEFAULT     = 8000
    LATENCY_MARGIN      = 150
)

func Bind(pod *apiv1.Pod, node *apiv1.Node, clientset *kubernetes.Clientset) {
    b := &apiv1.Binding{
            ObjectMeta: metav1.ObjectMeta{
                    Namespace: (*pod).Namespace,
                    Name:      (*pod).Name,
            },
            Target: apiv1.ObjectReference{
                    Kind: "Node",
                    Name: (*node).Name,
            },
    }
    corev1 := clientset.CoreV1()
    err := corev1.Pods((*pod).Namespace).Bind(b)
    if err != nil {
            log.Fatal(err)
    }
}

func FindNode(nodes apiv1.NodeList, requirement int) (*apiv1.Node, error) {
    for _, node := range nodes.Items {
        log.Println("Node labels")
        for key, val := range node.Labels {
            log.Printf(" %s %s\n", key, val)

            if key == "dpdk" {
                numOfIfs, err := strconv.Atoi(val)
                if err != nil {
                    return nil, err
                }
                log.Printf("Node %s with %d dpdp interfaces\n", node.Name, numOfIfs)
                switch requirement {
                case LATENCY_WITH_MULTUS:
                    if numOfIfs > 0 {
                        return &node, nil
                    }
                case LATENCY_WITH_HOPS:
                    if numOfIfs == 0 {
                        return &node, nil
                    }
                default:
                    return nil, errors.New("Cannot be scheduled")
                }
            }
        }
    }
    return nil, errors.New("Cannot be scheduled")
}

func FitPod(latency int, nodes apiv1.NodeList) (*apiv1.Node, error) {
    if latency < LATENCY_MARGIN+LATENCY_WITH_MULTUS {
            return FindNode(nodes, LATENCY_WITH_MULTUS)
    }
    return nil, errors.New("Not a proper latency value")
}

func SchedulePod(pod apiv1.Pod, nodes apiv1.NodeList, clientset *kubernetes.Clientset) {
    log.Printf("Scheduling pod \"%s\" to nodes\n", pod.Name)
    for _, node := range nodes.Items {
            log.Printf("\t%s\n", node.Name)
    }

    var keyFound bool = false
    for key, val := range pod.Annotations {
        if key == "latency" {
            latencyVal, err := strconv.Atoi(val)
            if err != nil {
                log.Fatal(err)
            }

            node, err := FitPod(latencyVal, nodes)
            if err != nil {
                log.Fatal(err)
            }

            Bind(&pod, node, clientset)
            log.Println(val)
            keyFound = true
        }
    }
    if !keyFound {
        log.Println("Pod annotation with [latency: value] needed")
    }
}

$ cat pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: busybox-latency
  annotations:
    latency : "100"
spec:  # specification of the pod's contents
  schedulerName: latency-scheduler
  containers:
  - name: busybox-latency-cont
    image: "busybox"
    command: ["top"]
    stdin: true
    tty: true

\end{lstlisting}

\section{Appendix B\label{LiiteB}}
The code written in Golang to check if a node has DPDK supported interfaces. This is run as a Daemon Set in Kubernetes.
\begin{lstlisting}[basicstyle={\small\ttfamily}]
$ cat main.go 
package main

import (
    "flag"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/clientcmd"
    "log"
    "os"
    "strconv"
)

func main() {
    var runInCluster = flag.Bool("run-in-cluster", false, "Run in cluster as a pod?")

    flag.Parse()

    var config *rest.Config
    var err error
    if *runInCluster {
            config, err = rest.InClusterConfig()
    } else {
        homeDir := os.Getenv("HOME")
        config, err = clientcmd.BuildConfigFromFlags("", homeDir+"/.kube/config")
    }
    if err != nil {
        log.Fatal(err)
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        log.Fatal(err)
    }

    corev1 := clientset.CoreV1()

    hostname, err := os.Hostname()
    if err != nil {
        log.Fatal(err)
    }

    node, err := corev1.Nodes().Get(hostname, metav1.GetOptions{})
    if err != nil {
        log.Fatal(err)
    }

    // Get interface details on the node
    ifcount, err := GetIfInfo()
    if err != nil {
        log.Fatal(err)
    }

    if node.Labels == nil {
        node.Labels = make(map[string]string)
    }
    node.Labels["dpdk"] = strconv.Itoa(ifcount)

    corev1.Nodes().Update(node)
}

$ cat if.go 
package main

import (
    "bufio"
    "os/exec"
    "strings"
)

func CheckNameIntel(line string) bool {
    validIfNames := []string{"82571", "82572", "82573", "82574",
        "82583", "ICH8", "ICH9", "ICH10", "PCH", "PCH2", "I217",
        "I218", "I219"}

    for _, name := range validIfNames {
            if strings.Contains(line, name) {
                    return true
            }
    }
    return false
}

// Returns only count to DPDK interfaces now
func GetIfInfo() (count int, err error) {
    lspciCmd := exec.Command("lspci")

    lspciOutput, err := lspciCmd.Output()
    if err != nil {
        return 0, err
    }

    var ifCount int = 0

    scanner := bufio.NewScanner(strings.NewReader(string(lspciOutput)))
    for scanner.Scan() {
        if strings.Contains(scanner.Text(), "Ethernet controller") &&
            strings.Contains(scanner.Text(), "Intel") {
            if CheckNameIntel(scanner.Text()) {
                ifCount++
            }
        }
    }
    return ifCount, nil
}

$ cat Dockerfile 
FROM golang:1.10.3

ADD . /go/src/github.com/alpha-black/k8s-node-iftype-discover
WORKDIR /go/src/github.com/alpha-black/k8s-node-iftype-discover
RUN go get "k8s.io/client-go/kubernetes"
RUN go get "k8s.io/client-go/rest"
RUN go install

$ cat ds.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-if-discovery
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: node-if-discovery
  template:
    metadata:
      labels:
        name: node-if-discovery
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: node-if-discovery-container
        image: k8s-node-iftype-ds:1.0
        command: ["./go/bin/k8s-node-iftype-discovery", "--logtostderr", "--run-in-cluster=true"]
        imagePullPolicy: IfNotPresent

\end{lstlisting}

\end{document}